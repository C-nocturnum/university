{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaqxhNAo3KgWi9OAf9yU18",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C-nocturnum/university/blob/main/New_Abliterated_Qwen_Coder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install and load\n",
        "!pip install huggingface_hub transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping datasets\n",
        "\n",
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import gc\n",
        "\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from jaxtyping import Float, Int\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3b6m-63FtGP",
        "outputId": "0c851f8f-96b2-49e2-9149-c1a12c71db89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: transformers_stream_generator in /usr/local/lib/python3.11/dist-packages (0.0.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.11/dist-packages (0.2.38)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (2.5.1+cu124)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.4.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.19.8)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping) (0.1.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (4.25.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turn automatic differentiation off to save GPU memory (credit: Undi95)\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpuF8Qewo1dD",
        "outputId": "c408545b-98cc-42b6-b17d-7cf9d907f400"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7d18251965d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reformat_texts(texts):\n",
        "    return [[{\"role\": \"user\", \"content\": text}] for text in texts]\n",
        "\n",
        "#get harmless dataset - update to C-Nocturnum folder for next iteration\n",
        "def get_harmless_instructions():\n",
        "    dataset = load_dataset('mlabonne/harmless_alpaca')\n",
        "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
        "\n",
        "#get harmful dataset - update to C-Nocturnum folder for next iteration\n",
        "def get_harmful_instructions():\n",
        "    dataset = load_dataset('mlabonne/harmful_behaviors')\n",
        "    return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text'])\n",
        "\n",
        "#train/test\n",
        "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
        "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
      ],
      "metadata": {
        "id": "XGo29KcrhCYW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set and download model to be abliterated\n",
        "MODEL_ID = \"Qwen/Qwen2.5-Coder-0.5B\"\n",
        "MODEL_TYPE = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "#download model from HF\n",
        "!git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeBh6GAuh78w",
        "outputId": "c361b568-8d5c-4710-92d4-f99c7b1c6035"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Qwen/Qwen2.5-0.5B' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load model & tokenizer\n",
        "model = HookedTransformer.from_pretrained_no_processing(\n",
        "    MODEL_TYPE,\n",
        "    local_files_only=True,\n",
        "    dtype=torch.bfloat16,\n",
        "    default_padding_side='left'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
        "tokenizer.padding_side = 'left'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1FZUIr-m3tl",
        "outputId": "dad1d67e-7b3c-4714-9c7d-fbaf51a9fe31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model Qwen/Qwen2.5-0.5B into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set to 256 to be manageable\n",
        "def tokenize_instructions(tokenizer, instructions):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        instructions,\n",
        "        padding=True,\n",
        "        truncation=False,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        add_generation_prompt=True,\n",
        "    ).input_ids\n",
        "\n",
        "n_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train))\n",
        "\n",
        "#tokenize datasets\n",
        "harmful_tokens = tokenize_instructions(\n",
        "    tokenizer,\n",
        "    instructions=harmful_inst_train[:n_inst_train],\n",
        ")\n",
        "harmless_tokens = tokenize_instructions(\n",
        "    tokenizer,\n",
        "    instructions=harmless_inst_train[:n_inst_train],\n",
        ")\n"
      ],
      "metadata": {
        "id": "rcFdGCRaiOmG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set batch size based on VRAM available\n",
        "batch_size = 32\n",
        "\n",
        "#initialize defaultdicts to store activations\n",
        "harmful = defaultdict(list)\n",
        "harmless = defaultdict(list)"
      ],
      "metadata": {
        "id": "qZts296niPt6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process training data in batches\n",
        "num_batches = (n_inst_train + batch_size - 1) // batch_size\n",
        "for i in tqdm(range(num_batches)):\n",
        "    print(i)\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min(n_inst_train, start_idx + batch_size)\n",
        "\n",
        "#run models on harmful/harmless prompts, then cache activations\n",
        "    harmful_logits, harmful_cache = model.run_with_cache(\n",
        "        harmful_tokens[start_idx:end_idx],\n",
        "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
        "        device='cpu',\n",
        "        reset_hooks_end=True\n",
        "    )\n",
        "    harmless_logits, harmless_cache = model.run_with_cache(\n",
        "        harmless_tokens[start_idx:end_idx],\n",
        "        names_filter=lambda hook_name: 'resid' in hook_name,\n",
        "        device='cpu',\n",
        "        reset_hooks_end=True\n",
        "    )\n",
        "\n",
        "#collect & store the activations\n",
        "    for key in harmful_cache:\n",
        "        harmful[key].append(harmful_cache[key])\n",
        "        harmless[key].append(harmless_cache[key])\n",
        "#flush VRAM & RAM\n",
        "    del harmful_logits, harmless_logits, harmful_cache, harmless_cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#concatenate the cached activations\n",
        "harmful = {k: torch.cat(v) for k, v in harmful.items()}\n",
        "harmless = {k: torch.cat(v) for k, v in harmless.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ezag_S3vWoH",
        "outputId": "f84a856e-5c0f-4606-94bd-780ddae1b794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function to get activation index\n",
        "def get_act_idx(cache_dict, act_name, layer):\n",
        "    key = (act_name, layer)\n",
        "    return cache_dict[utils.get_act_name(*key)]\n",
        "\n",
        "#step 2 - compute difference of means b/n harmful/harmless activations at intermediate layers\n",
        "activation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"]\n",
        "activation_refusals = defaultdict(list)\n",
        "\n",
        "for layer_num in range(1, model.cfg.n_layers):\n",
        "    pos = -1  # position index\n",
        "\n",
        "    for layer in activation_layers:\n",
        "        harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0)\n",
        "        harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean(\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        refusal_dir = harmful_mean_act - harmless_mean_act\n",
        "        refusal_dir = refusal_dir / refusal_dir.norm()\n",
        "        activation_refusals[layer].append(refusal_dir)\n",
        "\n",
        "# get all calculated potential refusal directions, sort them in descending order based on their mean\n",
        "# use a subset of layers if certain activations are not promising\n",
        "selected_layers = [\"resid_pre\"]\n",
        "activation_scored = sorted(\n",
        "    [\n",
        "        activation_refusals[layer][l - 1]\n",
        "        for l in range(1, model.cfg.n_layers)\n",
        "        for layer in selected_layers\n",
        "    ],\n",
        "    key=lambda x: abs(x.mean()),\n",
        "    reverse=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pCjUHMGriUf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_with_hooks(\n",
        "    model: HookedTransformer,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    tokens: Int[Tensor, \"batch_size seq_len\"],\n",
        "    max_tokens_generated: int = 64,\n",
        "    fwd_hooks=[],\n",
        ") -> List[str]:\n",
        "    all_tokens = torch.zeros(\n",
        "        (tokens.shape[0], tokens.shape[1] + max_tokens_generated),\n",
        "        dtype=torch.long,\n",
        "        device=tokens.device,\n",
        "    )\n",
        "    all_tokens[:, : tokens.shape[1]] = tokens\n",
        "    for i in range(max_tokens_generated):\n",
        "        with model.hooks(fwd_hooks=fwd_hooks):\n",
        "            logits = model(all_tokens[:, : -max_tokens_generated + i])\n",
        "            next_tokens = logits[:, -1, :].argmax(\n",
        "                dim=-1\n",
        "            )  # greedy sampling (temperature=0)\n",
        "            all_tokens[:, -max_tokens_generated + i] = next_tokens\n",
        "    return tokenizer.batch_decode(\n",
        "        all_tokens[:, tokens.shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "def get_generations(\n",
        "    model: HookedTransformer,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    instructions: List[str],\n",
        "    fwd_hooks=[],\n",
        "    max_tokens_generated: int = 64,\n",
        "    batch_size: int = 4,\n",
        ") -> List[str]:\n",
        "    generations = []\n",
        "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
        "        tokens = tokenize_instructions(\n",
        "            tokenizer, instructions=instructions[i : i + batch_size]\n",
        "        )\n",
        "        generation = _generate_with_hooks(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            tokens,\n",
        "            max_tokens_generated=max_tokens_generated,\n",
        "            fwd_hooks=fwd_hooks,\n",
        "        )\n",
        "        generations.extend(generation)\n",
        "    return generations\n",
        "\n",
        "#inference-time intervention hook\n",
        "def direction_ablation_hook(\n",
        "    activation: Float[Tensor, \"... d_act\"],\n",
        "    hook: HookPoint,\n",
        "    direction: Float[Tensor, \"d_act\"],\n",
        "):\n",
        "    if activation.device != direction.device:\n",
        "        direction = direction.to(activation.device)\n",
        "    proj = (\n",
        "        einops.einsum(\n",
        "            activation, direction.view(-1, 1), \"... d_act, d_act single -> ... single\"\n",
        "        )\n",
        "        * direction\n",
        "    )\n",
        "    return activation - proj\n",
        "\n",
        "#testing baseline\n",
        "N_INST_TEST = 4\n",
        "baseline_generations = get_generations(\n",
        "    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n",
        ")\n",
        "\n",
        "#evaluate layers defined earlier (review to work out best layer for refusal inhibition)\n",
        "EVAL_N = 20  # evaluate how many of the top N potential directions\n",
        "evals = []\n",
        "for refusal_dir in tqdm(activation_scored[:EVAL_N]):\n",
        "    hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir)\n",
        "    fwd_hooks = [\n",
        "        (utils.get_act_name(act_name, layer), hook_fn)\n",
        "        for layer in list(range(model.cfg.n_layers))\n",
        "        for act_name in activation_layers\n",
        "    ]\n",
        "    intervention_generations = get_generations(\n",
        "        model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks\n",
        "    )\n",
        "    evals.append(intervention_generations)\n"
      ],
      "metadata": {
        "id": "f4ZS2z2mia0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print generations for manual evaluation\n",
        "blacklist = [\"I cannot\", \"I can't\"]\n",
        "for i in range(N_INST_TEST):\n",
        "    print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\")\n",
        "    print(f\"\\nBASELINE COMPLETION:\\n{baseline_generations[i]}\\033[0m\")\n",
        "    for layer_candidate in range(EVAL_N):\n",
        "        if not any(word in evals[layer_candidate][i] for word in blacklist):\n",
        "            print(f\"\\n---\\n\\nLAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\")\n",
        "            print(evals[layer_candidate][i])\n"
      ],
      "metadata": {
        "id": "ic0ELnVziblb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_orthogonalized_matrix(\n",
        "    matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"]\n",
        ") -> Float[Tensor, \"... d_model\"]:\n",
        "    proj = (\n",
        "        einops.einsum(\n",
        "            matrix, vec.view(-1, 1), \"... d_model, d_model single -> ... single\"\n",
        "        )\n",
        "        * vec\n",
        "    )\n",
        "    return matrix - proj\n",
        "\n",
        "#select the layer with the highest potential refusal direction\n",
        "LAYER_CANDIDATE = 9\n",
        "refusal_dir = activation_scored[LAYER_CANDIDATE]\n",
        "\n",
        "#orthogonalize the model's weights (ie permanently abliterate model)\n",
        "if refusal_dir.device != model.W_E.device:\n",
        "    refusal_dir = refusal_dir.to(model.W_E.device)\n",
        "model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n",
        "\n",
        "for block in tqdm(model.blocks):\n",
        "    if refusal_dir.device != block.attn.W_O.device:\n",
        "        refusal_dir = refusal_dir.to(block.attn.W_O.device)\n",
        "    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n",
        "    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)\n",
        "\n",
        "#generate text with abliterated model\n",
        "orthogonalized_generations = get_generations(\n",
        "    model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[]\n",
        ")\n",
        "\n",
        "#print generations\n",
        "for i in range(N_INST_TEST):\n",
        "    if len(baseline_generations) > i:\n",
        "        print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\")\n",
        "        print(f\"\\033[92mBASELINE COMPLETION:\\n{baseline_generations[i]}\")\n",
        "    print(f\"\\033[91mINTERVENTION COMPLETION:\\n{evals[LAYER_CANDIDATE][i]}\")\n",
        "    print(f\"\\033[95mORTHOGONALIZED COMPLETION:\\n{orthogonalized_generations[i]}\\n\")\n"
      ],
      "metadata": {
        "id": "wFHrYwbCieJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert model back to HF safetensors\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16)\n",
        "lm_model = hf_model.model\n",
        "\n",
        "state_dict = model.state_dict()\n",
        "lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu())\n",
        "\n",
        "for l in range(model.cfg.n_layers):\n",
        "    lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter(\n",
        "        einops.rearrange(\n",
        "            state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=model.cfg.n_heads\n",
        "        ).contiguous()\n",
        "    )\n",
        "    lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter(\n",
        "        torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous()\n",
        "    )\n",
        "\n",
        "hf_model.push_to_hub(f\"{MODEL_ID}-abliterated\")\n",
        "# hf_model.push_to_hub(f\"{MODEL_ID}-abliterated\")\n"
      ],
      "metadata": {
        "id": "BkepKVULihxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}